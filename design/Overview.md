# The design of J-TEXT disruption database (JDDB)

## Purpose

The purpose of this DB is to facilitate smooth development of disruption predictors.

This will require the DB to have the following functions:

1. allow user to label the shots. labeling means give a set of key value pairs to the shot
2. allow user to query the DB to get a set of shots that meets the given criteria.
3. read shot data
4. process the data, let the user generate a set of processed data
5. performance evaluation and generate performance metric and statistics as well as basic figures.

## Structure of the JDDB

First we need to make sure the scope of the JDDB. JDDB can be used to facilitate a data processing pipeline. The data in it does not need to be the diagnostic data or raw data of the data generated directly with the experiment. It can contain processed data and extracted features or even neural network model output, AS LONG AS THE CONCEPT SHOT STILL APPLIES. The JDDB works with shots. If you are working with slices data or samples for you model or batches of data that not organized as shot, it will be considered as model specific data which is out side the scope of JDDB.

The JDDB is consists of 2 sub databases and a set of tools.

The sub databases are MetaDB and FileRepo.

### MetaDB

MetaDB is a MongoDB database with only one collection. In that collection each document represents a shot. The fields are the labels for that shot. Labels are key value pairs. Shown in the JSON code below, some keys have numeric values like IP, downtime, some have bool values. Those boolean value normally represents available signals.

```json
add a sample document here
```

The MetaDB can be hosted on a central server that everyone have read access to. It usually represent all the metadata that you can query from tokamak.

Since normally you are not allowed to modify the central MetaDB, you can of course get a dump of this MetaDB and restore it on your own machine. So that you can add more metadata that is specific to your study and query against them.

### FileRepo

File Repo is just a file repository on file system. It is set of HDF5 files organized in a folder structure that one can find a file for a shot easily by giving a shot NO.

Note that, an HDF5 file is a shot. No more no less. A shot won't be spread in multiple HDF5 files nor a file contain multiple shots.

Shot file should *always* named as **'shot_no.hdf5'** (such as 10000.hdf5).

Each file contain two groups, **'data'** and **'meta'**.

- 'Data' group:

   - Each *tag* (diagnostic raw data or processed data) should save in the **'data'** group as a dataset.

   - **Attributes** belong to each dataset, and by default, they should include the sampling rate named as **'SampleRate'** and start time named as **'StartTime'** of the data for reconstructing the time axis.
 
- 'Meta' group:

  - Each *label* should save in the **'meta'** group as a dataset.
  - No **attribute** belongs to the dataset in **'meta'** group. 
  - The labels can be synced from MongoDB service to the hdf5 file using MetaDB. We also provide an interface in FileRepo.

The HDF5 internal structure follows this format:

- shot_no.hdf5
  
  - data (group)
    
    - Ip (dataset)
    array(10000,)
      - SampleRate (attribute_1)
      - StartTime (attribute_2)
    - tag_2 (dataset)
      array(50000,)
      - SampleRate (attribute_1)
      - StartTime (attribute_2)  
  - meta (group)
  
    - Is_Disrupt (dataset)
    'True' (or 1)
    - label_2 (dataset)
    int() or float()


Here giving a HDF5 example:

The file have 2 groups one is data the other is meta. Meta is simple it is just a local backup of the labels in the MetaDB for this shot. It is there for quick and simple access when you don't need to query and filter shots.

The data group contains the actual data for this shot. You can put any data in here. Most of the time they are time series data either produced by diagnostics or generated by data processing tools. Each signal will have its basic attributes like start time and sample rate.

### Tools

Tools are python packages to work with the JDDB. See read me of the tools for usage.

## Tools over view

Here you get a basic overview of the tools. Not the detailed usages:

### JDDB.MetaDB

This module allow you to connect to a mongodb either central or your own local copy. It's mainly here to help you getting shot list according to your query or help modify the MetaDB to and more information.

### JDDB.FileRepo

This module allow you to work with local HDF5 file repo. it will get shot files when given shot numbers. It also give you primitive HDF5 reading functions. Another important function is to dump mdsplus data into a FileRepo.

### JDDB.Processor

This is a OO data processing tool that let you build you own data processing pipeline. the input and output of this tool is HDF5 shot file. It depends on JDDP.FileRepo. This is very useful for normalizing signals, resample signals and extracting features.

### JDDB.Performance

This provide tools to easily evaluate the performance of a model.
